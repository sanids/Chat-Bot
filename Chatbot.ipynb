{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanids/hello_world/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1cemrQCegcqk",
        "colab_type": "code",
        "outputId": "160886a0-cbe8-49a3-94c0-189ca9423d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Keras \n",
        "import keras.utils as ku \n",
        "# The Layers we will Use\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "# Tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# Our model\n",
        "from keras.models import Sequential\n",
        "# To address overfitting\n",
        "from keras.callbacks import EarlyStopping\n",
        "# To enable sequencing in our data\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# Seeds for reproducibility\n",
        "from tensorflow import set_random_seed\n",
        "from numpy.random import seed\n",
        "set_random_seed(2)\n",
        "seed(1)\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "aXzoHvMQidPS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BfJNilbJinxA",
        "colab_type": "code",
        "outputId": "03bcf666-fde2-477e-d02b-13f553e8547a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "cell_type": "code",
      "source": [
        "lines[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
              " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
              " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n",
              " 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n",
              " \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n",
              " 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n",
              " 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n",
              " 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "dYzTADS8ixS8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtysqIeWixWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "convs = [ ]\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nBifw4KBixad",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conversations = []\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        conversations.append(id2line[conv[i]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G9B2mvmvixde",
        "colab_type": "code",
        "outputId": "8666cf62-328a-44ba-94a2-0692fbc3f2ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(conversations[0])\n",
        "print(conversations[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XQ2naVfrjY0_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "\n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EE9trSTvjjpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clean_conversations = []\n",
        "for conv in conversations:\n",
        "  clean_conversations.append(clean_text(conv))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwOg8ZeDoCAv",
        "colab_type": "code",
        "outputId": "28b48a78-8730-4415-ec92-2b2bac6eb43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 3):\n",
        "    print(clean_conversations[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
            "well i thought we would start with pronunciation if that is okay with you\n",
            "not the hacking and gagging and spitting part  please\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXTtYj4JoCDx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare a tokenizer object to use\n",
        "tokenizer = Tokenizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W07Rr6iqoCGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sequences(text):\n",
        "    # encode our words\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    # how many words we have in total ( + 1 because it starts at 0)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "    ## convert data to sequence\n",
        "    sequences = []\n",
        "    for sentence in text:\n",
        "        token_sentences = tokenizer.texts_to_sequences([sentence])[0]\n",
        "        for i in range(1, len(token_sentences)):\n",
        "            # For each token (word) in our sentence we create an array with the token and its previous tokens\n",
        "            sequence = token_sentences[:i+1]\n",
        "            # Add that sequence to our array of sequences\n",
        "            sequences.append(sequence)\n",
        "    # Return our total sequences and the total number of words in our text\n",
        "    return sequences, total_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTXKIKUdoCJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "token_conv, total_words = get_sequences(clean_conversations[:35000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V2jpY6PLoCLi",
        "colab_type": "code",
        "outputId": "21951cdd-39ab-4ec1-b7ec-d75632353691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(token_conv[:5])\n",
        "print(clean_conversations[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[50, 18], [50, 18, 99], [50, 18, 99, 19], [50, 18, 99, 19, 885], [50, 18, 99, 19, 885, 9384]]\n",
            "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v74gSedeoCTl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def padded_sequences(sequences):\n",
        "  # So we extract the max length and use that one. Shorter sequences will just use 0's where they don't have words\n",
        "    max_sequence_length = max([len(x) for x in sequences])\n",
        "    # Now we have to reshape our sequences to fit to this new lentgh\n",
        "    # Thankfully keras has the function pad_sequences that does this\n",
        "    # We then make it an array by calling np.array(padded_sequences)\n",
        "    sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_length, padding='pre'))\n",
        "    \n",
        "    # Now we split our sequences into data and labels\n",
        "    # for the phrase \"hello new world\"\n",
        "    # we will have the seuqences and labels: \n",
        "    # hello -> new\n",
        "    # hello new -> world\n",
        "    # Where each label is the next word we are trying to predict based on the sequence\n",
        "    data = sequences[:,:-1]\n",
        "    # So our data will be all the words up to the last one\n",
        "    label = sequences[:,-1]\n",
        "    # Our label will be our last word\n",
        "    # We don't want to assign greater importance to certain words just because they have a bigger number\n",
        "    # So we make them all arrays of 0 and 1. \n",
        "    # Each one of our labels will have a specific value\n",
        "    # i.e, hello can be [0, 0, 0, ..... ,  1] \n",
        "    # the length depends on the number of words we can have\n",
        "    \n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return data, label, max_sequence_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FvcCZNTVrHzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "padded_conv, label_conv, max_sequence_length = padded_sequences(token_conv[:30000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGAKFwjlrH2L",
        "colab_type": "code",
        "outputId": "7693276e-40ce-4850-88c8-be0cd3a91c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "cell_type": "code",
      "source": [
        "print(padded_conv[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            " 50]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CjPggs1xrSni",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare a sequential model\n",
        "model = Sequential()\n",
        "# Add a layer to the model (Embedding) that will allow us to take the inputs\n",
        "model.add(Embedding(total_words, 10, input_length=max_sequence_length - 1)) # because its not 0-based\n",
        "# Add an LSTM Layer with 100 units\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "# Add another layer (our output layer) with softmax actiavtion\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "# Compile model with adam optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYFhrXe6rSq_",
        "colab_type": "code",
        "outputId": "d8c9ac4d-3d48-4136-b540-ecb855eef0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 217, 10)           184970    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               44400     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 18497)             1868197   \n",
            "=================================================================\n",
            "Total params: 2,097,567\n",
            "Trainable params: 2,097,567\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cwMNI3p-rcbU",
        "colab_type": "code",
        "outputId": "b6014545-a9ef-4f0b-d720-5989334ba71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(padded_conv, label_conv, epochs=100, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " - 217s - loss: 6.5688\n",
            "Epoch 2/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hs9WIA3euRws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, model, max_sequence_len):\n",
        "    for _ in range(50):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0)\n",
        "        \n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O4E3iMnvrcdt",
        "colab_type": "code",
        "outputId": "773313eb-ed12-42cd-a126-e95b74180f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "print (generate_text(\"What is the capital of Peru?\", model, max_sequence_length))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5e7691f96dae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the capital of Peru?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_text' is not defined"
          ]
        }
      ]
    }
  ]
}